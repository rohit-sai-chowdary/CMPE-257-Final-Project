{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:38.177053Z",
     "iopub.status.busy": "2021-12-06T04:33:38.176258Z",
     "iopub.status.idle": "2021-12-06T04:33:43.594933Z",
     "shell.execute_reply": "2021-12-06T04:33:43.594020Z",
     "shell.execute_reply.started": "2021-12-06T04:33:38.176966Z"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook should be run on kaggle competition ( \"I'm something of a painter myself\" )\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable the TPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:43.597000Z",
     "iopub.status.busy": "2021-12-06T04:33:43.596737Z",
     "iopub.status.idle": "2021-12-06T04:33:49.643801Z",
     "shell.execute_reply": "2021-12-06T04:33:49.643226Z",
     "shell.execute_reply.started": "2021-12-06T04:33:43.596972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable the TPU cluster resolver.\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "#### Requires turning on the Internet on the Settings tab to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:49.644988Z",
     "iopub.status.busy": "2021-12-06T04:33:49.644770Z",
     "iopub.status.idle": "2021-12-06T04:33:50.026264Z",
     "shell.execute_reply": "2021-12-06T04:33:50.025459Z",
     "shell.execute_reply.started": "2021-12-06T04:33:49.644962Z"
    }
   },
   "outputs": [],
   "source": [
    "KAGGLE_GCS_PATH = KaggleDatasets().get_gcs_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:50.029272Z",
     "iopub.status.busy": "2021-12-06T04:33:50.028949Z",
     "iopub.status.idle": "2021-12-06T04:33:50.184611Z",
     "shell.execute_reply": "2021-12-06T04:33:50.182952Z",
     "shell.execute_reply.started": "2021-12-06T04:33:50.029231Z"
    }
   },
   "outputs": [],
   "source": [
    "MONET_TFREC_IMAGE_FILES = tf.io.gfile.glob(str(KAGGLE_GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
    "PHOTOGRAPH_TFREC_IMAGE_FILES = tf.io.gfile.glob(str(KAGGLE_GCS_PATH + '/photo_tfrec/*.tfrec'))\n",
    "\n",
    "print(\"TFREC Records for Monet Images: %s\" % len(MONET_TFREC_IMAGE_FILES))\n",
    "print(\"TFREC Records for Photograph Images: %s\" % len(PHOTOGRAPH_TFREC_IMAGE_FILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:50.186872Z",
     "iopub.status.busy": "2021-12-06T04:33:50.186549Z",
     "iopub.status.idle": "2021-12-06T04:33:50.200527Z",
     "shell.execute_reply": "2021-12-06T04:33:50.198757Z",
     "shell.execute_reply.started": "2021-12-06T04:33:50.186831Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_image(photo):\n",
    "    photo = tf.image.decode_jpeg(photo, channels=3)\n",
    "    photo = (tf.cast(photo, tf.float32) / 127.5) - 1\n",
    "    photo = tf.reshape(photo, [256,256, 3])\n",
    "    return photo\n",
    "\n",
    "def tfrecord_read(sample):\n",
    "    tfrecord_format = {\"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "                       \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "                       \"target\": tf.io.FixedLenFeature([], tf.string)}\n",
    "    sample = tf.io.parse_single_example(sample, tfrecord_format)\n",
    "    image = decode_image(sample[\"image\"])\n",
    "    return image\n",
    "\n",
    "def load_dataset(names, labeled=True, ordered=False):\n",
    "    ds = tf.data.TFRecordDataset(names)\n",
    "    ds = ds.map(tfrecord_read, num_parallel_calls = AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset for the Model\n",
    "#### The format are tfrecords which can be cycled through an iterator. Unlike other methods, you cannot determine the length of the records without iterating through all the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:50.203694Z",
     "iopub.status.busy": "2021-12-06T04:33:50.203442Z",
     "iopub.status.idle": "2021-12-06T04:33:50.385969Z",
     "shell.execute_reply": "2021-12-06T04:33:50.384948Z",
     "shell.execute_reply.started": "2021-12-06T04:33:50.203659Z"
    }
   },
   "outputs": [],
   "source": [
    "monet_images_dataset = load_dataset(MONET_TFREC_IMAGE_FILES, labeled = True).batch(1)\n",
    "photographs_images_dataset = load_dataset(PHOTOGRAPH_TFREC_IMAGE_FILES, labeled = True).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:33:50.387526Z",
     "iopub.status.busy": "2021-12-06T04:33:50.387199Z",
     "iopub.status.idle": "2021-12-06T04:34:02.270270Z",
     "shell.execute_reply": "2021-12-06T04:34:02.269459Z",
     "shell.execute_reply.started": "2021-12-06T04:33:50.387486Z"
    }
   },
   "outputs": [],
   "source": [
    "num_records_monet = sum(1 for record in monet_images_dataset)\n",
    "num_records_photographs = sum(1 for record in photographs_images_dataset)\n",
    "print(\"# of monet images to train with: %s\" % num_records_monet)\n",
    "print(\"# of photographs to predict: %s\" % num_records_photographs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more photographs for the prediction than there are monet images. This is good, as the photographs transformations will be very useful in training and validating. The monet images are mostly for the model to learn patterns from and apply the transformation to the photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a sample of the Dataset\n",
    "#### Iterate through some of the records and show a pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:02.271765Z",
     "iopub.status.busy": "2021-12-06T04:34:02.271566Z",
     "iopub.status.idle": "2021-12-06T04:34:02.717740Z",
     "shell.execute_reply": "2021-12-06T04:34:02.717082Z",
     "shell.execute_reply.started": "2021-12-06T04:34:02.271740Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_monet_image = next(iter(monet_images_dataset))\n",
    "sample_photograph_image = next(iter(photographs_images_dataset))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Sample Original Photograph\")\n",
    "plt.imshow((sample_photograph_image[0] * 0.5) + 0.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Sample Monet Artwork\")\n",
    "plt.imshow((sample_monet_image[0] * 0.5) + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers for the GAN Model (downsampling, upsampling, Reflection Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the initializers used to feed the functions below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:02.719515Z",
     "iopub.status.busy": "2021-12-06T04:34:02.718805Z",
     "iopub.status.idle": "2021-12-06T04:34:02.724111Z",
     "shell.execute_reply": "2021-12-06T04:34:02.723267Z",
     "shell.execute_reply.started": "2021-12-06T04:34:02.719485Z"
    }
   },
   "outputs": [],
   "source": [
    "k_init = tf.random_normal_initializer(0., 0.02)\n",
    "g_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Downsampling, Upsampling and ConstantPadding2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:02.726663Z",
     "iopub.status.busy": "2021-12-06T04:34:02.726453Z",
     "iopub.status.idle": "2021-12-06T04:34:02.743138Z",
     "shell.execute_reply": "2021-12-06T04:34:02.742471Z",
     "shell.execute_reply.started": "2021-12-06T04:34:02.726639Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaddingConstant2D(layers.Layer):\n",
    "    def __init__(self, padding = (1,1), **kwargs):\n",
    "        self.pd = padding\n",
    "        super(PaddingConstant2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, input_tensor, mask = None):\n",
    "        pd_wth, pd_hgt = self.pd\n",
    "        pd_tensor = [[0,0], [pd_hgt, pd_hgt], [pd_wth, pd_wth], [0,0]]\n",
    "        return tf.pad(input_tensor, pd_tensor, mode=\"CONSTANT\")\n",
    "\n",
    "def downsampling_layers(model_layers, filters, activation, kernel_initializer=k_init , kernel_size=(3,3),\n",
    "                        strides=(2,2), padding=\"same\", gamma_initializer=g_init, use_bias=False):\n",
    "    # Conv2D -> Activation (optional)\n",
    "    model_layers = layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializer,\n",
    "                                 padding=padding, use_bias=use_bias)(model_layers)\n",
    "    \n",
    "    if activation:\n",
    "        model_layers = activation(model_layers)\n",
    "    return model_layers\n",
    "\n",
    "def upsampling_layers(model_layers, filters, activation, kernel_initializer=k_init, kernel_size=(3,3),\n",
    "                      strides=(2,2), padding=\"same\", gamma_initializer=g_init, use_bias=False):\n",
    "    # Conv2DTranspose -> Normalization -> Activation (optional)\n",
    "    model_layers = layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding, \n",
    "                                          kernel_initializer=kernel_initializer, use_bias=use_bias)(model_layers)\n",
    "    model_layers = tfa.layers.InstanceNormalization(gamma_initializer = gamma_initializer)(model_layers)\n",
    "    \n",
    "    if activation:\n",
    "        model_layers = activation(model_layers)\n",
    "    return model_layers\n",
    "\n",
    "def residual_block_layers(model_layers, activation, kernel_initializer=k_init,\n",
    "                          kernel_size=(3,3), strides=(1,1), padding=\"valid\",\n",
    "                          gamma_initializer=g_init, use_bias=False):\n",
    "    input_tensor = model_layers\n",
    "    num_output_filters = model_layers.shape[-1]\n",
    "\n",
    "    # Append all the layers here.\n",
    "    model_layers = PaddingConstant2D()(input_tensor)\n",
    "    model_layers = layers.Conv2D(num_output_filters, kernel_size, strides=strides, kernel_initializer=kernel_initializer,\n",
    "                                 padding=padding, use_bias=use_bias)(model_layers)\n",
    "    model_layers = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(model_layers)\n",
    "    model_layers = activation(model_layers)\n",
    "    model_layers = PaddingConstant2D()(model_layers)\n",
    "    model_layers = layers.Conv2D(num_output_filters, kernel_size, strides=strides, kernel_initializer=kernel_initializer,\n",
    "                                 padding=padding, use_bias=use_bias)(model_layers)\n",
    "    model_layers = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(model_layers)\n",
    "    model_layers = layers.add([input_tensor, model_layers])\n",
    "    return model_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:02.744861Z",
     "iopub.status.busy": "2021-12-06T04:34:02.744427Z",
     "iopub.status.idle": "2021-12-06T04:34:02.763637Z",
     "shell.execute_reply": "2021-12-06T04:34:02.762445Z",
     "shell.execute_reply.started": "2021-12-06T04:34:02.744832Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------- Generator Model\n",
    "def generator_model(filters=64, dwn_blocks=2, res_blocks=9, up_blocks=2,\n",
    "                    kernel_initializer=k_init, gamma_initializer=g_init,\n",
    "                    name=None):\n",
    "    image_layer_name = name + \"_image_input\"\n",
    "    image_input = layers.Input(shape=(256,256,3), name=image_layer_name)\n",
    "\n",
    "    # Define the Relu activation layer.\n",
    "    relu_activation_layer = layers.Activation(\"relu\")\n",
    "    \n",
    "    model_layers = PaddingConstant2D(padding=(3,3))(image_input)\n",
    "    model_layers = layers.Conv2D(filters, (7,7), kernel_initializer=kernel_initializer, use_bias=False)(model_layers)\n",
    "    model_layers = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(model_layers)\n",
    "    model_layers = relu_activation_layer(model_layers)\n",
    "    \n",
    "    # Add Downsampling layers\n",
    "    for _ in range(dwn_blocks):\n",
    "        filters *= 2\n",
    "        model_layers = downsampling_layers(model_layers, filters=filters, activation=relu_activation_layer)\n",
    "        \n",
    "    # Add Residual block layers\n",
    "    for _ in range(res_blocks):\n",
    "        model_layers = residual_block_layers(model_layers, activation=relu_activation_layer)\n",
    "        \n",
    "    # Add Upsampling layers\n",
    "    for _ in range(up_blocks):\n",
    "        filters //= 2\n",
    "        model_layers = upsampling_layers(model_layers, filters=filters, activation=relu_activation_layer)\n",
    "        \n",
    "    # Final layers with Tanh activation.\n",
    "    model_layers = PaddingConstant2D(padding=(3,3))(model_layers)\n",
    "    model_layers = layers.Conv2D(3, (7,7), padding=\"valid\")(model_layers)\n",
    "    model_layers = layers.Activation(\"tanh\")(model_layers)\n",
    "    \n",
    "    model = keras.models.Model(image_input, model_layers, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "#-------------- Discriminator Model\n",
    "def discriminator_model(filters=64, kernel_initializer=k_init, num_downsampling=3, name=None):\n",
    "    image_layer_name = name + \"_image_input\"\n",
    "    image_input = layers.Input(shape=(256,256,3), name=image_layer_name)\n",
    "    \n",
    "    # Define the leaky relu activation layer.\n",
    "    leaky_relu_activation_layer = layers.LeakyReLU(0.2)\n",
    "    \n",
    "    # Add a convolution layer with 2x2 strides followed by a leaky relu activation layer.\n",
    "    model_layers = layers.Conv2D(filters, (4,4), strides=(2,2), padding=\"same\", \n",
    "                                 kernel_initializer=kernel_initializer)(image_input)\n",
    "    model_layers = leaky_relu_activation_layer(model_layers)\n",
    "    \n",
    "    # Add Downsampling layers.\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            model_layers = downsampling_layers(model_layers, filters=num_filters, activation=leaky_relu_activation_layer,\n",
    "                                               kernel_size=(4,4), strides=(2,2))\n",
    "        else:\n",
    "            model_layers = downsampling_layers(model_layers, filters=num_filters, activation=leaky_relu_activation_layer,\n",
    "                                               kernel_size=(4,4),strides=(1,1))\n",
    "\n",
    "    # Finally add the convolution layer with 1x2 stride at the end of the model.\n",
    "    model_layers = layers.Conv2D(1, (4,4), strides=(1,1), padding=\"same\",\n",
    "                                 kernel_initializer=kernel_initializer)(model_layers)\n",
    "    model = keras.models.Model(image_input, model_layers, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable the TPU on the Generator and Discriminator Models and Model Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:02.765410Z",
     "iopub.status.busy": "2021-12-06T04:34:02.765065Z",
     "iopub.status.idle": "2021-12-06T04:34:09.821009Z",
     "shell.execute_reply": "2021-12-06T04:34:09.820318Z",
     "shell.execute_reply.started": "2021-12-06T04:34:02.765350Z"
    }
   },
   "outputs": [],
   "source": [
    "Adam_Optimizer = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "with strategy.scope():\n",
    "    # Define the generator and discriminator models\n",
    "    monet_generator = generator_model(name=\"generator_monet\")  # Transforms photos to monet\n",
    "    photo_generator = generator_model(name=\"generator_photo\")  # Transforms monet to photos\n",
    "    monet_discriminator = discriminator_model(name=\"discriminator_monet\") # Differentiates real monets and generated monets\n",
    "    photo_discriminator = discriminator_model(name=\"discriminator_photo\") # Differentiates real photos and generated photos\n",
    "    \n",
    "    # Define the optimizers for the models.\n",
    "    monet_generator_optimizer = Adam_Optimizer\n",
    "    photo_generator_optimizer = Adam_Optimizer\n",
    "    monet_discriminator_optimizer = Adam_Optimizer\n",
    "    photo_discriminator_optimizer = Adam_Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:09.822875Z",
     "iopub.status.busy": "2021-12-06T04:34:09.822552Z",
     "iopub.status.idle": "2021-12-06T04:34:10.551877Z",
     "shell.execute_reply": "2021-12-06T04:34:10.551297Z",
     "shell.execute_reply.started": "2021-12-06T04:34:09.822835Z"
    }
   },
   "outputs": [],
   "source": [
    "converted_monet_image = monet_generator(sample_photograph_image)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original Photograph\")\n",
    "plt.imshow((sample_photograph_image[0] * 0.5) + 0.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Generated Monet Artwork\")\n",
    "plt.imshow((converted_monet_image[0] * 0.5) + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is initialized and ready to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Loss Functions to be ran on the TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:10.553487Z",
     "iopub.status.busy": "2021-12-06T04:34:10.552780Z",
     "iopub.status.idle": "2021-12-06T04:34:10.563209Z",
     "shell.execute_reply": "2021-12-06T04:34:10.562464Z",
     "shell.execute_reply.started": "2021-12-06T04:34:10.553458Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def discriminator_loss(real_image, generated_image):\n",
    "        loss_real = keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                    reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real_image), real_image)\n",
    "        loss_fake = keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                    reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated_image), generated_image)\n",
    "\n",
    "        total_discriminator_loss = (loss_real + loss_fake) * 0.5\n",
    "        return total_discriminator_loss    \n",
    "\n",
    "    def generator_loss(generated_image):\n",
    "        gen_loss = keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                   reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated_image), generated_image)\n",
    "        return gen_loss\n",
    "\n",
    "    def cycle_loss(real_image, cycled_image, lamda):\n",
    "        loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "        cyc_loss = loss * lamda\n",
    "        return cyc_loss\n",
    "\n",
    "    def identity_loss(real_image, same_image, lamda1, lamda2):\n",
    "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "        id_loss = loss * lamda1 * lamda2\n",
    "        return id_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CycleGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:10.564966Z",
     "iopub.status.busy": "2021-12-06T04:34:10.564758Z",
     "iopub.status.idle": "2021-12-06T04:34:10.589623Z",
     "shell.execute_reply": "2021-12-06T04:34:10.588786Z",
     "shell.execute_reply.started": "2021-12-06T04:34:10.564942Z"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGAN(keras.Model):\n",
    "    def __init__(self, monet_generator, photo_generator,\n",
    "                 monet_discriminator, photo_discriminator,\n",
    "                 lambda_cycle=10.0, lambda_identity=0.5):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.monet_generator = monet_generator\n",
    "        self.photo_generator = photo_generator\n",
    "        self.monet_discriminator = monet_discriminator\n",
    "        self.photo_discriminator = photo_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "        \n",
    "    def compile(self, monet_generator_optimizer, photo_generator_optimizer,\n",
    "                monet_discriminator_optimizer, photo_discriminator_optimizer,\n",
    "                generator_loss_function, discriminator_loss_function,\n",
    "                cycle_loss_function, identity_loss_function):\n",
    "        super(CycleGAN, self).compile()\n",
    "        self.monet_generator_optimizer = monet_generator_optimizer\n",
    "        self.photo_generator_optimizer = photo_generator_optimizer\n",
    "        self.monet_discriminator_optimizer = monet_discriminator_optimizer\n",
    "        self.photo_discriminator_optimizer = photo_discriminator_optimizer\n",
    "        self.generator_loss = generator_loss_function\n",
    "        self.discriminator_loss = discriminator_loss_function\n",
    "        self.cycle_loss = cycle_loss_function\n",
    "        self.identity_loss = identity_loss_function\n",
    "    \n",
    "    # The naming convention below is shared by keras.Model per epoch.\n",
    "    def train_step(self, batch):\n",
    "        monet_real, photo_real = batch\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # photo --> monet --> photo\n",
    "            monet_fake = self.monet_generator(photo_real, training=True)\n",
    "            photo_cycled = self.photo_generator(monet_fake, training=True)\n",
    "            \n",
    "            # monet --> photo --> monet\n",
    "            photo_fake = self.photo_generator(monet_real, training=True)\n",
    "            monet_cycled = self.monet_generator(photo_fake, training=True)\n",
    "            \n",
    "            # Identity mapping. Attempt to reproduce the same source image for model accuracy comparison.\n",
    "            monet_approximate = self.monet_generator(monet_real, training=True)\n",
    "            photo_approximate = self.photo_generator(photo_real, training=True)\n",
    "            \n",
    "            # Discriminator outputs used for training.\n",
    "            discriminator_monet_real = self.monet_discriminator(monet_real, training=True)\n",
    "            discriminator_monet_fake = self.monet_discriminator(monet_fake, training=True)\n",
    "            discriminator_photo_real = self.photo_discriminator(photo_real, training=True)\n",
    "            discriminator_photo_fake = self.photo_discriminator(photo_fake, training=True)\n",
    "            \n",
    "            # Generator cycle losses\n",
    "            monet_cycled_loss = self.cycle_loss(monet_real, monet_cycled, self.lambda_cycle)\n",
    "            photo_cycled_loss = self.cycle_loss(photo_real, photo_cycled, self.lambda_cycle)\n",
    "            total_cycle_loss = monet_cycled_loss + photo_cycled_loss\n",
    "            \n",
    "            # Generator losses for the discriminator fake outputs.\n",
    "            monet_generator_loss = self.generator_loss(discriminator_monet_fake)\n",
    "            photo_generator_loss = self.generator_loss(discriminator_photo_fake)\n",
    "            \n",
    "            # Identity losses\n",
    "            monet_identity_loss = self.identity_loss(monet_real, monet_approximate,\n",
    "                                                     self.lambda_cycle, self.lambda_identity)\n",
    "            photo_identity_loss = self.identity_loss(photo_real, photo_approximate,\n",
    "                                                     self.lambda_cycle, self.lambda_identity)\n",
    "            total_identity_loss = monet_identity_loss + photo_identity_loss\n",
    "            \n",
    "            # Total generator losses\n",
    "            total_monet_generator_loss = monet_generator_loss + total_cycle_loss + total_identity_loss\n",
    "            total_photo_generator_loss = photo_generator_loss + total_cycle_loss + total_identity_loss\n",
    "            \n",
    "            # Discriminator losses\n",
    "            monet_discriminator_loss = self.discriminator_loss(discriminator_monet_real, discriminator_monet_fake)\n",
    "            photo_discriminator_loss = self.discriminator_loss(discriminator_photo_real, discriminator_photo_fake)\n",
    "        \n",
    "        # Calculate Gradients for Generator and Discriminators.\n",
    "        monet_generator_gradients = tape.gradient(total_monet_generator_loss,\n",
    "                                                  self.monet_generator.trainable_variables)\n",
    "        photo_generator_gradients = tape.gradient(total_photo_generator_loss,\n",
    "                                                  self.photo_generator.trainable_variables)\n",
    "        monet_discriminator_gradients = tape.gradient(monet_discriminator_loss,\n",
    "                                                      self.monet_discriminator.trainable_variables)\n",
    "        photo_discriminator_gradients = tape.gradient(photo_discriminator_loss,\n",
    "                                                      self.photo_discriminator.trainable_variables)\n",
    "        \n",
    "        self.monet_generator_optimizer.apply_gradients(zip(monet_generator_gradients,\n",
    "                                                           self.monet_generator.trainable_variables))\n",
    "        self.photo_generator_optimizer.apply_gradients(zip(photo_generator_gradients,\n",
    "                                                           self.photo_generator.trainable_variables))\n",
    "        self.monet_discriminator_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n",
    "                                                               self.monet_discriminator.trainable_variables))\n",
    "        self.photo_discriminator_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
    "                                                               self.photo_discriminator.trainable_variables))\n",
    "        \n",
    "        return {'monet_gen_loss': total_monet_generator_loss,\n",
    "                'photo_gen_loss': total_photo_generator_loss,\n",
    "                'monet_disc_loss': monet_discriminator_loss,\n",
    "                'photo_disc_loss': photo_discriminator_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the CycleGAN Model to run on the TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:10.591445Z",
     "iopub.status.busy": "2021-12-06T04:34:10.591119Z",
     "iopub.status.idle": "2021-12-06T04:34:10.659696Z",
     "shell.execute_reply": "2021-12-06T04:34:10.658766Z",
     "shell.execute_reply.started": "2021-12-06T04:34:10.591397Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGAN(monet_generator, photo_generator, monet_discriminator, photo_discriminator)\n",
    "    cycle_gan_model.compile(monet_generator_optimizer=monet_generator_optimizer,\n",
    "                            photo_generator_optimizer=photo_generator_optimizer,\n",
    "                            monet_discriminator_optimizer=monet_discriminator_optimizer,\n",
    "                            photo_discriminator_optimizer=photo_discriminator_optimizer,\n",
    "                            generator_loss_function=generator_loss,\n",
    "                            discriminator_loss_function=discriminator_loss,\n",
    "                            cycle_loss_function=cycle_loss,\n",
    "                            identity_loss_function=identity_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CycleGAN Model with 50 epochs\n",
    "The model fitting will be stuck at Epoch 1/50 for sometime while loading to the TPU, after sometime, the TPU will kickstart and the training will begin.\n",
    "\n",
    "With the TPU, each epoch takes approximately 2 minutes. This means 50 epochs takes about 120 minutes or about 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:34:10.661228Z",
     "iopub.status.busy": "2021-12-06T04:34:10.660997Z",
     "iopub.status.idle": "2021-12-06T04:38:43.022912Z",
     "shell.execute_reply": "2021-12-06T04:38:43.022090Z",
     "shell.execute_reply.started": "2021-12-06T04:34:10.661203Z"
    }
   },
   "outputs": [],
   "source": [
    "history = cycle_gan_model.fit(tf.data.Dataset.zip((monet_images_dataset, photographs_images_dataset)),\n",
    "                              epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Predictions from the monet_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:38:43.024616Z",
     "iopub.status.busy": "2021-12-06T04:38:43.024359Z",
     "iopub.status.idle": "2021-12-06T04:38:47.843486Z",
     "shell.execute_reply": "2021-12-06T04:38:47.842624Z",
     "shell.execute_reply.started": "2021-12-06T04:38:43.024587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show 10 photographs transformed to Monet artworks.\n",
    "_, axes = plt.subplots(10, 2, figsize=(20,20))\n",
    "for idx, photo_image in enumerate(photographs_images_dataset.take(10)):\n",
    "    # Make sure training=False so that we don't affect the trained generator.\n",
    "    monet_of_photo = monet_generator(photo_image, training=False)[0].numpy()\n",
    "    \n",
    "    # Scale the Monet generated output and input photograph.\n",
    "    monet_of_photo = (monet_of_photo * 0.5) + 0.5\n",
    "    photo_image = ((photo_image[0] * 0.5) + 0.5).numpy()\n",
    "    \n",
    "    # Show the comparison images.\n",
    "    axes[idx, 0].imshow(photo_image)\n",
    "    axes[idx, 1].imshow(monet_of_photo)\n",
    "    axes[idx, 0].set_title(\"Photograph Input\")\n",
    "    axes[idx, 1].set_title(\"Generated Monet Output\")\n",
    "    axes[idx, 0].axis(\"off\")\n",
    "    axes[idx, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Kaggle Submission\n",
    "#### First make a directory to store the images from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:38:47.845416Z",
     "iopub.status.busy": "2021-12-06T04:38:47.844684Z",
     "iopub.status.idle": "2021-12-06T04:38:48.651140Z",
     "shell.execute_reply": "2021-12-06T04:38:48.649906Z",
     "shell.execute_reply.started": "2021-12-06T04:38:47.845353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make an images directory.\n",
    "! mkdir ../images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the monet generated outputs from monet_generator and save them into the images folder. Finally, zip it.\n",
    "#### Use the TPU to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T04:38:48.653970Z",
     "iopub.status.busy": "2021-12-06T04:38:48.652973Z",
     "iopub.status.idle": "2021-12-06T05:20:18.653697Z",
     "shell.execute_reply": "2021-12-06T05:20:18.652813Z",
     "shell.execute_reply.started": "2021-12-06T04:38:48.653911Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    count = 1\n",
    "    for photo_image in photographs_images_dataset:\n",
    "        monet_of_photo = monet_generator(photo_image, training=False)[0].numpy()\n",
    "        monet_of_photo = ((monet_of_photo * 127.5) + 127.5).astype(np.uint8)\n",
    "        monet_image = PIL.Image.fromarray(monet_of_photo)\n",
    "        monet_image.save(\"../images/\" + str(count) + \".jpg\")\n",
    "        count += 1\n",
    "        if (count % 101 == 0):\n",
    "            print(\"Generated Monet of photograph for photo = %s/%s\" % (count, num_records_photographs))\n",
    "    \n",
    "# Make an archive in /kaggle/working/images as images.zip\n",
    "shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
